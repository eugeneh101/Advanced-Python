{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Performance Python\n",
    "Once in awhile, you will hear that \"oh Python is so slow or not performant enough\" or \"Python is not for <i>enterprise</i> use cases since it is not fast enough\" or \"Python is a language for script kiddies and at best \"glue\" code.\"  \n",
    "ZOMG! Scala's functional API is awesome and stands on the shoulders of the JVM. Go(lang) has goroutines that beats the pants off of Python's multithreading. Rust--I don't know what it does but it sounds trendy. ðŸ™„ \n",
    "\n",
    "Spare me the faux concern ~~you pretentious C++ developer~~ ... I mean good question! Python supports rapid speed of development and experimentation, expressiveness and conciseness of syntax, and high-performance frameworks for speed. At a high-level, speed up Python performance on 1 machine using vectorization, multi-threading, multi-processing, and asynchronous programming. On multiple machines, you have cluster/distributed computing frameworks: Apache Spark, Dask, Apache Beam, Ray, and others. On cloud services, you have containers and scalable, serverless computing. In this tutorial, I will show you various ways to make Python fast and scalable, so performance is not your bottleneck.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debunking the \"Slow\" Python Myth\n",
    "What do you mean by slow? Do you mean that Python runs slow, so it would cost more money when renting EC2s?  \n",
    "Remember that compute time is cheap and developer time is relatively expensive. Suppose a standard laptop has 4 CPUs and 16 GB of RAM. An EC2 instance on AWS rents for just \\\\$0.154 per hour; an engineer costs \\\\$50 per hour. So for each hour of developer time you waste, you could have rented a machine for 300 hours--or you can rent 300 machines for 1 hour. A 40 hour week for on EC2 would be \\$6--cheaper than your daily lunch!  \n",
    "Some \"faster\" languages are notoriously verbose, so you end up with pages and pages of code. Python's ease of use and expressivenes gives you the super power: rapid speed of development gives you the ability to beat your competition to the market. Rapid experimentation allows you to out-maneuver your competition with better and newer features. Python's readability offers maintainability since developers come and go. Basically, can you write your code faster than somebody else: can you do in 1 hour that somebody else takes 5?  \n",
    "\n",
    "If you need speed, Guido recommends you delegate the performance critical part to Cython:  \n",
    "```\"At some point, you end up with one little piece of your system, as a whole, where you end up spending all your time. If you write that just as a sort of simple-minded Python loop, at some point you will see that that is the bottleneck in your system. It is usually much more effective to take that one piece and replace that one function or module with a little bit of code you wrote in C or C++ rather than rewriting your entire system in a faster language, because for most of what you're doing, the speed of the language is irrelevant.\" --Guido van Rossum```  \n",
    "I won't mention Cython in the tutorial since it is rarely ever used--I have never personally met anybody who wrote their custom Cython code. Things that need to be performant in Python are already written in Cython: numpy, XGBoost, SpaCy, etc. The high level idea for Cython is that Python is actually written on top of C, so for speed you can write directly in C (or C++) and import it to Python.  \n",
    "\n",
    "If you are concerned about speed, do you know what is faster than Python? Fortran.  Then you can brush up on your Fortran to get hired by ... nobody (or worse, mainframe specialists ðŸ¤¢).  \n",
    "\n",
    "Speed of the language primarily helps with CPU bound problems and not IO bound problems--in IO bound problems, any programming language will perform roughly the same since the majority of the time, you are waiting for a network response.  \n",
    "\n",
    "Finally, you don't really care about the raw speed of a language. You care about the expressioness and functionality of the language: its ecosystem, user-base, and mindshare. Whatever problem you can think of, Python probably has a library for that. Which you can pip install. Which you can write minimal code. With which you solve your problem. So you can call it a day. Like Newton, when you use Python, you stand on the shoulders of giants. Things just work! Imagine if you couldn't just import sklearn and run Random Forest but had to write your own random forest algorithm. Actually, don't...  \n",
    "Here are some libraries just for scientific computing.  \n",
    "<p align=\"center\"><img src=\"images/scientific-ecosystem.png\" width=600></p><br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import antigravity # a cute Easter egg; things in Python are so easy. Python has \"batteries included\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big O, Not Just Japanese Batman\n",
    "<p align=\"center\"><img src=\"images/The-Big-O.jpg\" width=300></p><br>  \n",
    "Data structures and algorithms (ie your functions) have different performance characteristics that can be measured by how much RAM it takes to do something and how steps it takes to do something. Fancy people use the terms <i>space and time complexity</i>, respectively. Don't be intimidated by the fancy names; I assure you it's a simple idea.  \n",
    "<b>Big O notation</b>: is used to denote the worst case scenario of how things will grow. \n",
    "\n",
    "\n",
    "Let's walk through some simple examples.  \n",
    "<i>list</i>:\n",
    "    * to append to the end of a list, it is (an amoritized) O(1) operation. Adding to a list\n",
    "\n",
    "to insert to the beginning of a list, then it takes O(n) operations because the element has to be \n",
    "\n",
    "\n",
    ", which is pronounced oh-enn.\n",
    "\n",
    "deque\n",
    "runtime vs space complexity  \n",
    "Galvanize technical interview with anagrams  \n",
    "adding strings together vs joining, adding list vs tuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized Calculations with SIMD\n",
    "vectorization with numpy and pandas: SIMD  \n",
    "like R's apply. pd.DataFrame.apply is not vectorized but without the speed performance. It's a convenience function.  \n",
    "CPU vs GPU: ALU vs FPU  \n",
    "order of magnitude of speed: CPU, RAM, storage, network: have image  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-threading vs Multi-processing\n",
    "threads (concurrency) (race condition, tug of war) vs processes (parallel): concurrent.futures vs multiprocessing  \n",
    "concurrency vs parallelism vs asynchronous (event driven)  \n",
    "give example of how the HBase was too slow and that Python was not the bottleneck--it was an IO problem    \n",
    "ETLed 150 TB of data on 1 machine WITHOUT Spark  \n",
    "GIL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "\n",
    "def append_and_pop(list_append, list_pop, iterations, checkpoint_iterations):\n",
    "    for i in range(iterations):\n",
    "        list_append.append(list_pop.pop())\n",
    "        if i % checkpoint_iterations == 0:\n",
    "            print(\n",
    "                \"length of `list_append`: {}; length of `list_pop` {}\"\n",
    "                .format(len(list_append), len(list_pop))\n",
    "            )\n",
    "    print(\n",
    "        \"final length of `list_append`: {}; final length of `list_pop` {}\"\n",
    "        .format(len(list_append), len(list_pop))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of `list_append`: 10000001; length of `list_pop` 9999999\n",
      "length of `list_append`: 9891816; length of `list_pop` 10108183\n",
      "length of `list_append`: 10134060; length of `list_pop` 9865940\n",
      "length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "length of `list_append`: 10116257; length of `list_pop` 9883743length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "length of `list_append`: 10006301; length of `list_pop` 9993699length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "length of `list_append`: 10052019; length of `list_pop` 9947981length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "length of `list_append`: 10016316; length of `list_pop` 9983684length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "length of `list_append`: 10134521; length of `list_pop` 9865479length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "length of `list_append`: 10320389; length of `list_pop` 9679611\n",
      "length of `list_append`: 9877198; length of `list_pop` 10122801\n",
      "length of `list_append`: 10189475; length of `list_pop` 9810525\n",
      "length of `list_append`: 9860034; length of `list_pop` 10139966\n",
      "length of `list_append`: 10052350; length of `list_pop` 9947650length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "final length of `list_append`: 10067202; final length of `list_pop` 9932798final length of `list_append`: 10000000; final length of `list_pop` 10000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1 = list(range(10000000))\n",
    "l2 = list(range(10000000))\n",
    "\n",
    "thread1 = Thread(target=append_and_pop, args=(l1, l2, 10000000, 1000000))\n",
    "thread2 = Thread(target=append_and_pop, args=(l2, l1, 10000000, 1000000))\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "\n",
      "Counter({True: 9896565, False: 103435})\n",
      "Counter({True: 9700888, False: 299112})\n",
      "\n",
      "{9954872, 9992673, 9974936, 9926734}\n",
      "{9957249, 9878581, 9799822, 9999999}\n",
      "\n",
      "[(9799822, 2), (9878581, 2), (9957249, 2), (9999999, 2), (0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "[(9974936, 2), (9954872, 2), (9926734, 2), (9992673, 2), (0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(l1 == list(range(10000000)))\n",
    "print(l2 == list(range(10000000)))\n",
    "\n",
    "print()\n",
    "\n",
    "print(Counter(i == j for i, j in zip(l1, range(10000000))))\n",
    "print(Counter(i == j for i, j in zip(l2, range(10000000))))\n",
    "\n",
    "print()\n",
    "\n",
    "print(set(range(10000000)) - set(l1))\n",
    "print(set(range(10000000))  - set(l2))\n",
    "\n",
    "print()\n",
    "\n",
    "print(Counter(l1).most_common(10))\n",
    "print(Counter(l2).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incrementer(lst, iterations, checkpoint_iterations):\n",
    "    for i in range(iterations):\n",
    "        lst[0] += 1\n",
    "        if i % checkpoint_iterations == 0:\n",
    "            print(\"lst: {}\".format(lst))\n",
    "    print(\"final lst: {}\".format(lst))\n",
    "\n",
    "def decrementer(lst, iterations, checkpoint_iterations):\n",
    "    for i in range(iterations):\n",
    "        lst[0] -= 1\n",
    "        if i % checkpoint_iterations == 0:\n",
    "            print(\"lst: {}\".format(lst))\n",
    "    print(\"final lst: {}\".format(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lst: [1]\n",
      "lst: [233888]\n",
      "lst: [796536]\n",
      "lst: [403365]\n",
      "lst: [1195976]\n",
      "lst: [1519278]\n",
      "lst: [1996572]\n",
      "lst: [2254294]lst: [2279738]\n",
      "\n",
      "lst: [2484877]\n",
      "lst: [2708386]\n",
      "lst: [2635015]\n",
      "lst: [2652956]\n",
      "lst: [3076750]\n",
      "lst: [2708854]\n",
      "lst: [2642597]\n",
      "lst: [2410366]\n",
      "lst: [2779173]\n",
      "lst: [1697253]\n",
      "lst: [2030757]\n",
      "final lst: [2285048]\n",
      "final lst: [1030758]\n"
     ]
    }
   ],
   "source": [
    "lst = [0]\n",
    "\n",
    "thread1 = Thread(target=incrementer, args=(lst, 10000000, 1000000))\n",
    "thread2 = Thread(target=decrementer, args=(lst, 10000000, 1000000))\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Programming/Event-Driven Programming\n",
    "asyncio is in some sense serial with indirection of a coroutine. asynchronous immplemented in 3 ways: callback (hell from nested structure), futures/promises (looks like [flat] method chaining) and flatter, async/await  \n",
    "yield from  \n",
    "you don't need asyncio to do asynchronous programming  \n",
    "C10k problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Computing Frameworks\n",
    "A common lesson of high performance computing: <i>High performance computing isnâ€™t about doing one thing exceedingly well, itâ€™s about doing nothing poorly.</i> Spark, Dask (dataframe, bag, array, futures, delayed), Beam, Ray, \n",
    "amdahl's law  \n",
    "vertical vs horizontal scaling  \n",
    "\n",
    "### MapReduce (old news)\n",
    "explain what is MapReduce and disadvantages  \n",
    "A `reduce` operation that is commutative and associative can be partially parallelized using a technique call <i>combiner</i>.  \n",
    "Map reduce on-premise cannot be scalled up, so EMR on AWS gives you the ability to scale up when needed. Even Spark cluster is apportioned and probably isn't ideal for overallocation.  \n",
    "\n",
    "## Apache Spark\n",
    "If you want to get a high-demand skill to get a new job, learn Spark. I always say: `If you know Spark, you can't get fired!`\n",
    "Simple Spark  \n",
    "Spark and Dask: often a functional approach, functoolz syntax translates to Dask bags, FP allows embarrasingly parallel and laziness.  \n",
    "Show Simple-Spark\n",
    "\n",
    "## Dask\n",
    "`If you learn Dask, then you know Spark. And if you know Spark, you can't get fired!`\n",
    "Compare and contrast  \n",
    "Show Dask tutorial  \n",
    "x = x.map_blocks(compress).persist().map_blocks(decompress)\n",
    "\n",
    "### Apache Beam\n",
    "explain why it is cool: unified API btween Batch + Stream, auto-scaling, serverless, templates  \n",
    "explain its weaknesses: does not support data with schema, no graph algorithms  \n",
    "Through picture in. 62 machines running for less than 30 minutes for cost of $2.50.\n",
    "\n",
    "### Ray (and Actor Model)\n",
    "\n",
    "\n",
    "# Virtual Machines vs Containers\n",
    "VM vs Docker/containerization, multi-tenancy  \n",
    "serverless computing, scalable/decoupled, notifications and PubSub connected to AWS Lambda  \n",
    "world has moved on from hardware -> co-location -> virtualization (rented VM) -> container -> function: https://read.acloud.guru/the-evolution-from-servers-to-functions-21833b576744  \n",
    "Focus on your business logic and delegate the rest to AWS. Who knows (and who cares) how its really implemented? Are you in the business of creating your private cloud/Hadoop or solving business problems that make $$$?  \n",
    "Can mention Pub/Sub and object notifications  \n",
    "\n",
    "\n",
    "# What is AWS?\n",
    "AWS is really supply chain maximization, squeezing out efficiencies from elementary pieces: compute, storage, and networking. With these Lego blocks, they can create managed services that are just a copy of a Apache/open-source software and charge for it. Overall win-win scenario. Like Costco or Trade Joe model \n",
    "Show all the logos of services from AWS or GCP. Show the open source version and then AWS service  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Resources\n",
    "Raymond Hettinger, Keynote on Concurrency, PyBay 2017: https://www.youtube.com/watch?v=9zinZmE3Ogk  \n",
    "https://pybay.com/site_media/slides/raymond2017-keynote/threading.html  \n",
    "https://glyph.twistedmatrix.com/2014/02/unyielding.html  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
