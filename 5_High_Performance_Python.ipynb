{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Performance Python\n",
    "Once in awhile, you will hear that \"oh Python is so slow or not performant enough\" or \"Python is not for <i>enterprise</i> use cases since it is not fast enough\" or \"Python is a language for script kiddies and at best \"glue\" code.\"  \n",
    "ZOMG! Scala's functional API is awesome and stands on the shoulders of the JVM. Go(lang) has goroutines that beats the pants off of Python's multithreading. Rust--I don't know what it does but it sounds trendy. ðŸ™„ \n",
    "\n",
    "Spare me the faux concern ~~you pretentious C++ developer~~ ... I mean good question! Python supports rapid speed of development and experimentation, expressiveness and conciseness of syntax, and high-performance frameworks for speed. At a high-level, speed up Python performance on 1 machine using vectorization, multi-threading, multi-processing, and asynchronous programming. On multiple machines, you have cluster/distributed computing frameworks: Apache Spark, Dask, Apache Beam, Ray, and others. On cloud services, you have containers and scalable, serverless computing. In this tutorial, I will show you various ways to make Python fast and scalable, so performance is not your bottleneck.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debunking the \"Slow\" Python Myth\n",
    "What do you mean by slow? Do you mean that Python runs slow, so it would cost more money when renting EC2s?  \n",
    "Remember that compute time is cheap and developer time is relatively expensive. Suppose a standard laptop has 4 CPUs and 16 GB of RAM. The equivalent EC2 instance on AWS rents for just \\\\$0.154 per hour; an engineer costs \\\\$50 per hour. So for each hour of developer time you waste, you could have rented a machine for 300 hours--or you can rent 300 machines for 1 hour. A 40 hour week for on EC2 would be \\$6--cheaper than your daily lunch!  \n",
    "Some \"faster\" languages are notoriously verbose, so you end up with pages and pages of code. Python's ease of use and expressivenes gives you a super power: rapid speed of development, which gives you a second super power: the ability to beat your competition to the market. Way of the Pro! Rapid experimentation allows you to out-maneuver your competition with better and newer features. Python's readability offers maintainability since developers come and go. Basically, can you write your code faster than somebody else: can you do something in 1 hour that somebody else takes 5?  \n",
    "\n",
    "If you need speed, Guido recommends you delegate the performance critical part to Cython:  \n",
    "```\"At some point, you end up with one little piece of your system, as a whole, where you end up spending all your time. If you write that just as a sort of simple-minded Python loop, at some point you will see that that is the bottleneck in your system. It is usually much more effective to take that one piece and replace that one function or module with a little bit of code you wrote in C or C++ rather than rewriting your entire system in a faster language, because for most of what you're doing, the speed of the language is irrelevant.\" --Guido van Rossum```  \n",
    "I won't mention Cython in the tutorial since it is rarely ever used--I have never personally met anybody who wrote their custom Cython code. Things that need to be performant in Python are already written in Cython: numpy, XGBoost, SpaCy, etc. The high level idea for Cython is that Python is actually written on top of C, so for speed you can write directly in C (or C++) and import it to Python.  \n",
    "\n",
    "If you are concerned about speed, do you know what is faster than Python? Fortran.  Then you can brush up on your Fortran to get hired by ... nobody (or worse, mainframe specialists ðŸ¤¢).  \n",
    "\n",
    "Speed of the language primarily helps with CPU bound problems and not IO bound problems--in IO bound problems, any programming language will perform roughly the same since the majority of the time, you are waiting for a network response.  \n",
    "\n",
    "Finally, you don't really care about the raw speed of a language. You care about the expressioness and functionality of the language: its ecosystem, user-base, and mindshare. Whatever problem you can think of, Python probably has a library for that. Which you can pip install. So you can write minimal code. With which you solve your problem. So you can call it a day. Like Newton, when you use Python, you stand on the shoulders of giants. Things just work! Imagine if you couldn't just import sklearn and run Random Forest but had to write your own random forest algorithm. Actually, don't...  \n",
    "Here are some libraries just for scientific computing.  \n",
    "<p align=\"center\"><img src=\"images/scientific-ecosystem.png\" width=600></p><br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import antigravity  # a cute Easter egg; things in Python are so easy. Python has \"batteries included\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big O, Not Just Japanese Batman\n",
    "<p align=\"center\"><img src=\"images/The-Big-O.jpg\" width=300></p>\n",
    "\n",
    "Data structures and algorithms (ie your functions) have different performance characteristics that can be measured by how much memory (space) it takes to do something and how many steps (time) it takes to do something. Fancy people use the terms <i>space and (run)time complexity</i>, respectively. Don't be intimidated by the fancy names; I assure you it's a simple idea.  \n",
    "<b>Big O notation</b>: is used to denote the worst case scenario of how resource utilization (memory or number of steps) will grow with respect to the number of elements in your input data.  \n",
    "Note: O(n) is pronounced oh-enn.\n",
    "\n",
    "### Data Structure Complexity\n",
    "Let's walk through some simple examples.  \n",
    "list:\n",
    "* to append to the end of a list, it is (an amoritized) O(1) operation. It is a fast operation.\n",
    "* to append/insert to the beginning of a list, it takes O(n) operations because a new list is created, new element is inserted, and all the elements from the original list is copied over. It is slow in comparison to append to the right.\n",
    "* to check if an element is in a list (ie test for membership), it takes O(n) operations since the worst case is that  element is not in the list, so you have to check every element of the list.\n",
    "* to get an element at a known, specific ith index, it is an O(1) operation. Fast\n",
    "\n",
    "dict:\n",
    "* to check if a key is in a dictionary: O(1) operation because dict keys are hashable (and unique).\n",
    "* to check if a value is in a dictinoary: O(n) operations because dict values are not hashable (and do not have to be unique).\n",
    "\n",
    "\n",
    "Ideally all your code takes as little memory and as few steps as possible (ie O(1) for instantenous results). In practice, code uses non-trivial amounts of memory and computation. Sometimes, you can consider a tradeoff between space and time requirements. For example, if you are using a list and often have to append to the front and the end of the list, then you are better off with a data structure called double-ended queue (AKA deque). A deque have rapid `append()` and `appendleft()` methods as well as `pop()` and `popleft()` as these operations are all O(1). However, a deque has slow access to the ith element, which is now a O(n) operation. You have to figure out which data structure gives you the best benefit given tradeoffs or constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Complexity\n",
    "As with data structures, algorithms also have space and time complexity. An algorithm is just a fancy word for a set of steps to get you to the right answer: sorting algorithm, sudoku solver algorithm, determining the winner in a tournament algorithm.\n",
    "\n",
    "When an engineer says something is *(computationally) cheap*, that is fancy lingo for if the calculation uses little resources (low space or time complexity) and is fast. If a calculation is *(computationally) expensive*, it means it takes a lot of memory or is slow. Ideally you want your algorithm to be cheap.\n",
    "\n",
    "There are orders of magnitudes for complexity of algorithms (and data structures):\n",
    "* `O(1)` (AKA constant time): the best (that you can possibly do)!\n",
    "* `O(log(n))` (AKA logarithmic time): good\n",
    "* `O(n)` (AKA linear time): still good\n",
    "* `O(n*log(n))`: fair, use sparingly\n",
    "* `O(n^2)` (AKA quadratic time): bad, use very sparingly. If the exponent is any number other than 2 (ie 3, 4, ...), then it is called polynomial time\n",
    "* `O(2^n)` (AKA exponential time) and `O(n!)`: horrible, ideally you never have use an algorithm (or data structure) that has these complexities because it will be slow and resource intensive.\n",
    "    \n",
    "Basically, you want your algorithm (or data structure) to *scale* well with the size of your input data. To visualize how important the scaling factor, Big O, is to your algorithm, here's a chart. Stay in the green.\n",
    "<p align=\"center\"><img src=\"images/Big_O_magnitudes.png\" width=800></p><br>  \n",
    "The chart and a more elaborate explanation of Big O is available at: <a href=\"https://towardsdatascience.com/understanding-time-complexity-with-python-examples-2bda6e8158a7\">Understanding time complexity with Python examples</a>\n",
    "\n",
    "Here is simple algorithm that concatenates strings together: it exhibits quadratic, O(n^2) time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      "['i', 'h', 'b', 's', 'v', 'm', 'U', 'q', 's', 'J']\n"
     ]
    }
   ],
   "source": [
    "from string import ascii_letters\n",
    "from random import choices\n",
    "\n",
    "print(ascii_letters)\n",
    "lots_of_letters = choices(ascii_letters, k=100000)\n",
    "print(lots_of_letters[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 171 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# functional style: quadratic time\n",
    "from functools import reduce\n",
    "\n",
    "result_FP = reduce(lambda accumulated_string, char: accumulated_string + char, lots_of_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# procedural style: quadratic time\n",
    "result_procedural = \"\"\n",
    "for char in lots_of_letters:\n",
    "    result_procedural += char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a smarter option that gives you linear, O(n) time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_OOP = \"\".join(lots_of_letters)  # OOP is much faster since runtime is O(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_FP == result_procedural == result_OOP  # proof that the results are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Smart data structures and dumb code works a lot better than the other way around.\" -from *The Cathedral and the Bazaar* by Eric S. Raymond\n",
    "\n",
    "When possible, pick a good data structure that fits your problem--because there are a lot of data structures that do cool and useful things. Using the right data structure is the low-hanging fruit that requires minimum effort. If your logic is fundamentally complicated, then you will have to get creative and write (and optimize) a smart algorithm--that is more work. Of course, the quote is simply a rule of thumb--right most of the time, but not all the time. Use good judgment.\n",
    "\n",
    "Let me give you a practical example. Often in technical interview questions, the interviewer will ask you to write/whiteboard an algorithm. This is an interview question I received many years ago when I was a beginner: \"Given a list of words, find which words are anagrams of each other.\" Here is my very crude solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rats': {'arts', 'star'}, 'arts': {'star'}, 'god': {'dog'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_anagrams(list_of_words):\n",
    "    anagrams = {}\n",
    "    for i, current_word in enumerate(list_of_words):\n",
    "        for following_word in list_of_words[i+1:]:\n",
    "            if Counter(current_word) == Counter(following_word):\n",
    "                if current_word in anagrams:\n",
    "                    anagrams[current_word].add(following_word)\n",
    "                else:\n",
    "                    anagrams[current_word] = {following_word}\n",
    "    return anagrams\n",
    "\n",
    "find_anagrams([\"rats\", \"arts\", \"star\", \"god\", \"dog\", \"apple\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My algorithm is very inefficient since it has O(n^2) time complexity: it has a double for loop. And it is duplicating the letter counts (ie calling *Counter()*) multiple times for the same word. Finally, the output isn't great.\n",
    "\n",
    "The correct way (Way of the Pro) is to find some way to transform the data into what I call a \"canonical form\" or \"canonical representation\" of the data that is invariant to differences that don't matter. For example, in this anagram problem, it does not matter what order the letters are in the word. Hence, to get the \"canonical form\", you sort the letters in the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rats', 'arts', 'star'], ['god', 'dog']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_anagrams_optimized(list_of_words):\n",
    "    anagrams = defaultdict(list)\n",
    "    for word in list_of_words:\n",
    "        word_with_letters_sorted = \"\".join(sorted(word))\n",
    "        anagrams[word_with_letters_sorted].append(word)\n",
    "    return [val for val in anagrams.values() if len(val) > 1]\n",
    "\n",
    "find_anagrams_optimized([\"rats\", \"arts\", \"star\", \"god\", \"dog\", \"apple\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that I used both:\n",
    "* a better data structure: default dict that allows easy appending vs a regular dict\n",
    "* a better algorithm: that has linear O(n) time complexity instead of O(n^2). No redundant, repeated computations\n",
    "\n",
    "Moreover, the new function uses half the lines of code to get better results, so it is more readable and maintainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Calculations with SIMD\n",
    "vectorization with numpy and pandas: SIMD  \n",
    "like R's apply. pd.DataFrame.apply is not vectorized but without the speed performance. It's a convenience function.  \n",
    "CPU vs GPU: ALU vs FPU  \n",
    "order of magnitude of speed: CPU, RAM, storage, network: have image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-threading vs Multi-processing\n",
    "threads (concurrency) (race condition, tug of war) vs processes (parallel): concurrent.futures vs multiprocessing, shared vs independent memory  \n",
    "GIL: multi threading makes function calls non-atomic. Not even functional programming can save you now.    \n",
    "concurrency vs parallelism vs asynchronous (event driven)  \n",
    "give example of how the HBase was too slow and that Python was not the bottleneck--it was an IO problem    \n",
    "My dirty little secret: ETLed 150 TB of data on 1 machine WITHOUT Spark  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "\n",
    "def append_and_pop(list_append, list_pop, iterations, checkpoint_iterations):\n",
    "    for i in range(iterations):\n",
    "        list_append.append(list_pop.pop())\n",
    "        if i % checkpoint_iterations == 0:\n",
    "            print(\n",
    "                \"length of `list_append`: {}; length of `list_pop` {}\"\n",
    "                .format(len(list_append), len(list_pop))\n",
    "            )\n",
    "    print(\n",
    "        \"final length of `list_append`: {}; final length of `list_pop` {}\"\n",
    "        .format(len(list_append), len(list_pop))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of `list_append`: 10000001; length of `list_pop` 9999999\n",
      "length of `list_append`: 9891816; length of `list_pop` 10108183\n",
      "length of `list_append`: 10134060; length of `list_pop` 9865940\n",
      "length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "length of `list_append`: 10116257; length of `list_pop` 9883743length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "length of `list_append`: 10006301; length of `list_pop` 9993699length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "length of `list_append`: 10052019; length of `list_pop` 9947981length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "length of `list_append`: 10016316; length of `list_pop` 9983684length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "length of `list_append`: 10134521; length of `list_pop` 9865479length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "length of `list_append`: 10320389; length of `list_pop` 9679611\n",
      "length of `list_append`: 9877198; length of `list_pop` 10122801\n",
      "length of `list_append`: 10189475; length of `list_pop` 9810525\n",
      "length of `list_append`: 9860034; length of `list_pop` 10139966\n",
      "length of `list_append`: 10052350; length of `list_pop` 9947650length of `list_append`: 10000000; length of `list_pop` 10000000\n",
      "\n",
      "final length of `list_append`: 10067202; final length of `list_pop` 9932798final length of `list_append`: 10000000; final length of `list_pop` 10000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l1 = list(range(10000000))\n",
    "l2 = list(range(10000000))\n",
    "\n",
    "thread1 = Thread(target=append_and_pop, args=(l1, l2, 10000000, 1000000))\n",
    "thread2 = Thread(target=append_and_pop, args=(l2, l1, 10000000, 1000000))\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "\n",
      "Counter({True: 9896565, False: 103435})\n",
      "Counter({True: 9700888, False: 299112})\n",
      "\n",
      "{9954872, 9992673, 9974936, 9926734}\n",
      "{9957249, 9878581, 9799822, 9999999}\n",
      "\n",
      "[(9799822, 2), (9878581, 2), (9957249, 2), (9999999, 2), (0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "[(9974936, 2), (9954872, 2), (9926734, 2), (9992673, 2), (0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(l1 == list(range(10000000)))\n",
    "print(l2 == list(range(10000000)))\n",
    "\n",
    "print()\n",
    "\n",
    "print(Counter(i == j for i, j in zip(l1, range(10000000))))\n",
    "print(Counter(i == j for i, j in zip(l2, range(10000000))))\n",
    "\n",
    "print()\n",
    "\n",
    "print(set(range(10000000)) - set(l1))\n",
    "print(set(range(10000000))  - set(l2))\n",
    "\n",
    "print()\n",
    "\n",
    "print(Counter(l1).most_common(10))\n",
    "print(Counter(l2).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incrementer(lst, iterations, checkpoint_iterations):\n",
    "    for i in range(iterations):\n",
    "        lst[0] += 1\n",
    "        if i % checkpoint_iterations == 0:\n",
    "            print(\"lst: {}\".format(lst))\n",
    "    print(\"final lst: {}\".format(lst))\n",
    "\n",
    "def decrementer(lst, iterations, checkpoint_iterations):\n",
    "    for i in range(iterations):\n",
    "        lst[0] -= 1\n",
    "        if i % checkpoint_iterations == 0:\n",
    "            print(\"lst: {}\".format(lst))\n",
    "    print(\"final lst: {}\".format(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lst: [1]\n",
      "lst: [233888]\n",
      "lst: [796536]\n",
      "lst: [403365]\n",
      "lst: [1195976]\n",
      "lst: [1519278]\n",
      "lst: [1996572]\n",
      "lst: [2254294]lst: [2279738]\n",
      "\n",
      "lst: [2484877]\n",
      "lst: [2708386]\n",
      "lst: [2635015]\n",
      "lst: [2652956]\n",
      "lst: [3076750]\n",
      "lst: [2708854]\n",
      "lst: [2642597]\n",
      "lst: [2410366]\n",
      "lst: [2779173]\n",
      "lst: [1697253]\n",
      "lst: [2030757]\n",
      "final lst: [2285048]\n",
      "final lst: [1030758]\n"
     ]
    }
   ],
   "source": [
    "lst = [0]\n",
    "\n",
    "thread1 = Thread(target=incrementer, args=(lst, 10000000, 1000000))\n",
    "thread2 = Thread(target=decrementer, args=(lst, 10000000, 1000000))\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Programming/Event-Driven Programming\n",
    "asyncio is in some sense serial with indirection of a coroutine. asynchronous immplemented in 3 ways: callback (hell from nested structure), futures/promises (looks like [flat] method chaining) and flatter, async/await  \n",
    "yield from  \n",
    "yield and return in the same function in a coroutine  \n",
    "you don't need asyncio to do asynchronous programming  \n",
    "C10k problem  \n",
    "basically a queue--asynchronous because not executed immediately. The event loop is the buffer that manages queue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computing Frameworks\n",
    "A common lesson of high performance computing: High performance computing isnâ€™t about doing one thing exceedingly well, itâ€™s about doing nothing poorly.  \n",
    "Spark, Dask (dataframe, bag, array, futures, delayed), Beam, Ray, \n",
    "amdahl's law  \n",
    "vertical vs horizontal scaling  \n",
    "In my opinion, distributed computing frameworks are glorified map/filter/reduce operations. Basically you want to chain a bunch of transformations together. If you have just 1 transformation, you will be better off with `multiprocessing`. You have a lot of elements (basically a distributed list) and then you want to do 1 thing to it, get an output, and then do another thing to that output. That's where functional programming comes in--how you approach the problem. The transformations are applied lazily, so they don't use memory until you want to materialize/trigger the actual computation.  \n",
    "\n",
    "### MapReduce (old news)\n",
    "explain what is MapReduce and disadvantages  \n",
    "Out-of-core-computation (often mistakenly called out of memory computation): enables large matrix multiplications and querying large databases such as Hive query executed with MapReduce  \n",
    "A `reduce` operation that is commutative and associative can be partially parallelized using a technique call <i>combiner</i>.  \n",
    "Map reduce on-premise cannot be scalled up, so EMR on AWS gives you the ability to scale up when needed. Even Spark cluster is apportioned and probably isn't ideal for overallocation.  \n",
    "show mapreduce stage picture  \n",
    "\n",
    "### Apache Spark (industry standard ðŸ†)\n",
    "If you want to get a high-demand skill to get a new job, learn Spark. I always say: `If you know Spark, you can't get fired!`  \n",
    "Spark is basically MapReduce in memory. The bottleneck in large scale computation is getting the data to the CPU. RAM is way faster than disk; order of magnitudes, get picture of CPU cache, RAM, disk, network  \n",
    "Simple Spark  \n",
    "Spark and Dask: often a functional approach, functoolz syntax translates to Dask bags, FP allows embarrasingly parallel and laziness.  \n",
    "Don't need Scala for speed  \n",
    "Weird Java exceptions  \n",
    "Show Simple-Spark  \n",
    "Weakness: no ability to do multiple actions. The workaround is to split pipeline into 2 such as word count and character simultaneously and then do groupby.  \n",
    "DataFrame, RDD\n",
    "show Spark architecture picture  \n",
    "\n",
    "### Dask (new framework with growing community adoption ðŸŒ±)\n",
    "Distributed + Task  \n",
    "`If you learn Dask, then you know Spark. And if you know Spark, you can't get fired!`  \n",
    "Compare and contrast  \n",
    "No weird Java exceptions  \n",
    "Show Dask tutorial (tree aggregation example)  \n",
    "delayed and futures  \n",
    "x = x.map_blocks(compress).persist().map_blocks(decompress)  \n",
    "Can do multiple actions  \n",
    "DataFrame, array, bag, delayed\n",
    "show Dask architecture picture (basically the same thing)  \n",
    "\n",
    "### Apache Beam (theoretically interesting but nobody uses ðŸ˜ž)\n",
    "explain why it is cool: unified API btween Batch + Stream, auto-scaling, serverless, templates, can do multiple actions  \n",
    "pcollections, ParDo  \n",
    "explain its weaknesses: does not support data with schema, no graph algorithms like K-Means, also no concept of action so have to use trick to trigger finality  \n",
    "Throw picture in: 62 machines running for 30 minutes cost \\$2.50.  \n",
    "\n",
    "Spark, Dask, Beam  \n",
    "Features:  \n",
    "* **batch**: first classs, first class, first class\n",
    "* stream: first class, kinda, first class\n",
    "* **real time Dashboard (for debugging and throughput)**: yes, yes, yes\n",
    "* **interactive mode/REPL**: yes, yes, yes\n",
    "* **persist (memory and disk)**: memory+disk, only memory, no\n",
    "* **with persist, can execute different sections independently**: yes, yes, no\n",
    "* **can split 1 input to multiple output pipelines**: yes, yes, yes\n",
    "* **simultaneous actions on multiple output pipelines**: no, yes, yes\n",
    "* support for branching logic of pipeline: yes, yes, yes\n",
    "* support for iterative algorithms: yes, yes, no\n",
    "* **GROUP BY logic**: yes, yes, yes\n",
    "* **JOIN logic**: yes, yes, yes\n",
    "* **tagged output pipelines**: no, no, yes\n",
    "* **combiner/aggregate logic**: yes, yes, yes\n",
    "* **SQL support**: yes, kinda, no\n",
    "* distributed numpy array: no, yes, no\n",
    "* ML algorithms: yes, yes, no\n",
    "* futures: no, yes, no\n",
    "* delayed: no, yes, no\n",
    "* support to address \"hot keys\": yes, yes, yes\n",
    "* **shared variables**: broad variable and accumulators, no, idk\n",
    "* supports logging for debugging: yes, yes, yes\n",
    "* **supports multiple input/output file types**: yes, yes, yes\n",
    "* click-based or GUI-based template: no, no, yes\n",
    "* **auto scaling**: idk, idk, yes\n",
    "\n",
    "Other support:\n",
    "* integration: many, growing, primarily only GCP\n",
    "* pure Python (3): no, yes, yes\n",
    "* **easy to run/test on your own machine**: yes, yes, yes\n",
    "* performant local cluster on your machine: idk, yes, no\n",
    "* speed: Scala (fast), Python (less fast), Python (less fast)\n",
    "* **documentation**: great, great, lacking\n",
    "* user base: big, medium, low\n",
    "\n",
    "\n",
    "Which one to use: Spark is a very hot data engineering skill. I tell my teammates: \"if you know Spark, then you can't be fired.\" Since I work on Dask, I also tell my teammates: \"if you know Dask, then you know Spark. If you know Spark, then you can't be fired.\"  \n",
    "\n",
    "\n",
    "### Ray (and Actor Model)\n",
    "like a blend of functional and OOP because it is stateful between transformations. Like Dask futures  \n",
    "Plasma -> Apache Arrow    \n",
    "totally novel architecture that overcomes IPC/RPC latency  \n",
    "show Ray architecture picture  \n",
    "\n",
    "\n",
    "\n",
    "These things are not built in a vacuum: progress stands on the shoulders of giants. Their histories are intertwined. Spark is basically mapreduce but in memory for the speed. Apache Beam originated from Google's internal Millwheel project and was marketed as true streaming. Spark basically copied Beam's approach. Dask was created originally for distributed arrays because Spark doesn't support it. Ray uses Redis and was probably inspired by Akka (framework in Scala). Dask adopted Ray's actor model. Basically copy ideas that work and dump the rest. A lot of frameworks that don't make the cut and just fade out. Other frameworks: Apache Apex (dead), Apache Gearpump (basically dead), Apache Samza (~~lame~~ only Java), Apache Flink (seems like a Hadoop thing, so basically for legacy applications)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Machines vs Containers\n",
    "VM vs Docker/containerization, multi-tenancy  \n",
    "serverless computing, scalable/decoupled, notifications and (asynchronous) PubSub connected to AWS Lambda (in some sense distributed but independent multiprocessing)  \n",
    "world has moved on from hardware -> co-location -> virtualization (rented VM) -> container -> function: https://read.acloud.guru/the-evolution-from-servers-to-functions-21833b576744  \n",
    "Focus on your business logic and delegate the rest to AWS. Who knows (and who cares) how its really implemented? Are you in the business of creating your private cloud/Hadoop or solving business problems that make $$$?  \n",
    "\n",
    "\n",
    "## What is the Cloud?\n",
    "There's an old saying: \"Nobody gets fired for buying IBM\". That was decades ago. I think the modern version would be \"Nobody gets fired for buying AWS\". The cloud is so versatile with all the services builtin, that if you are still buying hardware for your own datacenters without good reasons, you *should* be fired. One of the clients I worked on is such an old-boys club that they still buy hardware, explain story.  \n",
    "AWS is really supply chain maximization, squeezing out efficiencies from elementary pieces: compute, storage, and networking. With these Lego blocks, they can create managed services that are just a copy of a Apache/open-source software and charge for it. The cloud doesn't cost you money--it saves you money. Overall win-win scenario. Like Costco or Trade Joe model \n",
    "Show all the logos of services from AWS or GCP. Show the open source version and then AWS service  \n",
    "\n",
    "Why do you buy a car? Because it is better, faster, and/or cheaper than you can make it yourself. It just \"works out of the box.\" You could build a car yourself; it would be worse functionality, slower to get to market, more expensive build from scratch, and a nightmare to maintain. Just buy it off-the-shelf. Focus on your main problem. Don't roll your own--you must be high if you do. Plus, if something breaks, there's somebody you can ~~complain to~~ ask for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Resources\n",
    "Raymond Hettinger, Keynote on Concurrency, PyBay 2017: https://www.youtube.com/watch?v=9zinZmE3Ogk  \n",
    "https://pybay.com/site_media/slides/raymond2017-keynote/threading.html  \n",
    "About asynchornous programming from the creator of Twisted and key influencer to asyncio: https://glyph.twistedmatrix.com/2014/02/unyielding.html  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
